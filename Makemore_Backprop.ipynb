{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 8586183,
          "sourceType": "datasetVersion",
          "datasetId": 5135485
        }
      ],
      "dockerImageVersionId": 30732,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Makemore_Backprop",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kshitij-1008/NN-micrograd-and-makemore/blob/main/Makemore_Backprop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'karpathy-makemore-names-text:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5135485%2F8586183%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240621%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240621T165654Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D80238bf59301524f042933cf4df74ba313465356d34f0f897af32a6b846d325ff3a63c842d3d5dbb2a5715a2bbd0461385bcbd842536d99b01b985ca70dd239c45f7d2b5c19f252885226f27b158a297ece2845c3f705326b1ee04d5b508d488cc62b6efe8b5a84aed256270c8bb0655eec34f3145dd374dc05d73466e5b2650b82d58214d61668c7030fbdaac5fb5c02c93b08b3c5ae66c6d9df0d5c520631483c6c83accc8dbc807b9ace3ea5598ce8033052a2bea24c0295df737af681911f356debe883ee914651f7fee08ca5c985ab9e147fe22962d9e3e70fd7ebc70f69a4c78863cc4018ff31e31c5cf4393e623efebc0bc1d2959de76aeab26637c5d'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "SLAOj-YKutQH"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-06-21T16:29:02.397403Z",
          "iopub.execute_input": "2024-06-21T16:29:02.398873Z",
          "iopub.status.idle": "2024-06-21T16:29:02.409982Z",
          "shell.execute_reply.started": "2024-06-21T16:29:02.398821Z",
          "shell.execute_reply": "2024-06-21T16:29:02.408574Z"
        },
        "trusted": true,
        "id": "aIpaI0qSutQN",
        "outputId": "d09623ff-74d5-4a2d-ab5f-c022ed09b016"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/karpathy-makemore-names-text/names.txt\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:29:02.444275Z",
          "iopub.execute_input": "2024-06-21T16:29:02.444798Z",
          "iopub.status.idle": "2024-06-21T16:29:02.452127Z",
          "shell.execute_reply.started": "2024-06-21T16:29:02.444739Z",
          "shell.execute_reply": "2024-06-21T16:29:02.450582Z"
        },
        "trusted": true,
        "id": "kYvoZSrWutQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation of the dataset"
      ],
      "metadata": {
        "id": "L2EwCzmQutQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# normal qualities of the txt file contents\n",
        "words = open(r'/kaggle/input/karpathy-makemore-names-text/names.txt', 'r').read().splitlines()\n",
        "print(len(words))\n",
        "print(words[:10])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:29:02.490066Z",
          "iopub.execute_input": "2024-06-21T16:29:02.490617Z",
          "iopub.status.idle": "2024-06-21T16:29:02.505244Z",
          "shell.execute_reply.started": "2024-06-21T16:29:02.490576Z",
          "shell.execute_reply": "2024-06-21T16:29:02.503516Z"
        },
        "trusted": true,
        "id": "HBwkUekfutQW",
        "outputId": "ecdf9aeb-69dc-4309-def8-00f93b58914d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "32033\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mappings from characters to integers and vice versa\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "print(itos)\n",
        "vocab_size = len(stoi)\n",
        "print(vocab_size)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:29:02.53925Z",
          "iopub.execute_input": "2024-06-21T16:29:02.540603Z",
          "iopub.status.idle": "2024-06-21T16:29:02.555791Z",
          "shell.execute_reply.started": "2024-06-21T16:29:02.540554Z",
          "shell.execute_reply": "2024-06-21T16:29:02.554564Z"
        },
        "trusted": true,
        "id": "nQCHFOT8utQZ",
        "outputId": "9eeb3fc8-f2e8-4511-a2fa-eb52267a8c0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the dataset"
      ],
      "metadata": {
        "id": "nXyQZA4butQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 3 #no. of characters to determine to next character in line.\n",
        "\n",
        "def build_dataset(words):\n",
        "    X,Y = [], []\n",
        "\n",
        "    for w in words: #first eg: 'emma'\n",
        "        context = [0]*block_size\n",
        "\n",
        "        for char in w + '.': #first eg: 'e+m+m+a+.'\n",
        "            idx = stoi[char]\n",
        "            Y.append(idx)\n",
        "            X.append(context)\n",
        "\n",
        "            context = context[1:]+[idx]\n",
        "\n",
        "    X= torch.tensor(X)\n",
        "    Y = torch.tensor(Y)\n",
        "    print(X.shape, Y.shape)\n",
        "    return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr, Ytr = build_dataset(words[:n1])\n",
        "Xval, Yval = build_dataset(words[n1:n2])\n",
        "Xtest, Ytest = build_dataset(words[n2:])\n",
        "\n",
        "Xtr[:5], Ytr[:5]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:29:02.597264Z",
          "iopub.execute_input": "2024-06-21T16:29:02.598013Z",
          "iopub.status.idle": "2024-06-21T16:29:03.996071Z",
          "shell.execute_reply.started": "2024-06-21T16:29:02.597976Z",
          "shell.execute_reply": "2024-06-21T16:29:03.994671Z"
        },
        "trusted": true,
        "id": "cJ1juGM2utQb",
        "outputId": "65608f67-e8d6-47c7-da4a-93bd6d523733"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "torch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n",
          "output_type": "stream"
        },
        {
          "execution_count": 334,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(tensor([[ 0,  0,  0],\n         [ 0,  0, 25],\n         [ 0, 25, 21],\n         [25, 21,  8],\n         [21,  8,  5]]),\n tensor([25, 21,  8,  5, 14]))"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the parameters"
      ],
      "metadata": {
        "id": "NlGy9ctWutQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_embed = 10\n",
        "n_hidden = 64\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) #for deterministic and reproducable results\n",
        "C = torch.randn((vocab_size, n_embed),           generator=g)\n",
        "#Layer 1\n",
        "W1 = torch.randn((n_embed*block_size, n_hidden), generator=g) * (5/3)/((n_embed*block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden, generator=g) * 0.1\n",
        "#Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),         generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                     generator=g) * 0.1\n",
        "#BatchNorm parameters\n",
        "bngain = torch.ones((1, n_hidden))*0.1+1.0\n",
        "bnbias = torch.zeros((1, n_hidden))*0.1\n",
        "\n",
        "# Note: initialization of many of these parameters has been done in non-standard ways\n",
        "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
        "# implementation of the backward pass\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters))\n",
        "for p in parameters:\n",
        "    p.requires_grad = True"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:29:03.998643Z",
          "iopub.execute_input": "2024-06-21T16:29:03.99913Z",
          "iopub.status.idle": "2024-06-21T16:29:04.012706Z",
          "shell.execute_reply.started": "2024-06-21T16:29:03.999094Z",
          "shell.execute_reply": "2024-06-21T16:29:04.011198Z"
        },
        "trusted": true,
        "id": "TOYkX77tutQc",
        "outputId": "d52dc169-48f8-414a-b1c3-95195bb88a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "4137\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility function"
      ],
      "metadata": {
        "id": "jz4D9TP2utQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function to compare gradients calculated manually and using PyTorch\n",
        "def cmp(s, dt, t):\n",
        "    ex = torch.all(dt==t.grad).item()\n",
        "    app = torch.allclose(dt, t.grad)\n",
        "    maxdiff = (dt-t.grad).abs().max().item()\n",
        "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:29:04.014403Z",
          "iopub.execute_input": "2024-06-21T16:29:04.014899Z",
          "iopub.status.idle": "2024-06-21T16:29:04.024305Z",
          "shell.execute_reply.started": "2024-06-21T16:29:04.014852Z",
          "shell.execute_reply": "2024-06-21T16:29:04.022654Z"
        },
        "trusted": true,
        "id": "L2fAtyxvutQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "Xb, Yb = Xtr[ix], Ytr[ix]\n",
        "Xb[:5], Yb[:5]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:29:04.028109Z",
          "iopub.execute_input": "2024-06-21T16:29:04.028619Z",
          "iopub.status.idle": "2024-06-21T16:29:04.042615Z",
          "shell.execute_reply.started": "2024-06-21T16:29:04.028572Z",
          "shell.execute_reply": "2024-06-21T16:29:04.04084Z"
        },
        "trusted": true,
        "id": "2Oss_TTRutQe",
        "outputId": "4e59bc08-36fa-4a12-d0f5-0c4af02ab210"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 337,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(tensor([[ 1,  1,  4],\n         [18, 14,  1],\n         [11,  5,  9],\n         [ 0,  0,  1],\n         [12, 15, 14]]),\n tensor([ 8, 14, 15, 22,  0]))"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass, edited to be able to manually calculate backward pass\n",
        "\n",
        "emb = C[Xb]\n",
        "emb_cat = emb.view(emb.shape[0], -1) #reshape the tensor\n",
        "\n",
        "#Layer 1\n",
        "h_pre_bn = emb_cat @ W1 + b1 # pre activation and batch normalization\n",
        "\n",
        "#BatchNorm Layer\n",
        "bnmeani = (1/batch_size)*h_pre_bn.sum(0, keepdim=True)   # mean of the batch\n",
        "bndiff = h_pre_bn - bnmeani                              # difference between batch and batch mean\n",
        "bndiff2 = bndiff**2                                      # square of the difference between batch and mean for variance\n",
        "bnvari = 1/(batch_size-1) * bndiff2.sum(0, keepdim=True) # variance\n",
        "bnvar_inv = (bnvari+1e-5)**(-0.5)                        # epsilon = 1e-5\n",
        "bnraw = bndiff*bnvar_inv\n",
        "h_pre_act = bngain * bnraw + bnbias\n",
        "#Non-Linearity\n",
        "h = torch.tanh(h_pre_act)\n",
        "\n",
        "#Layer 2\n",
        "logits = h @ W2 + b2\n",
        "\n",
        "#cross-entropy loss\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes  #for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1\n",
        "probs = counts * counts_sum_inv\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(batch_size), Yb].mean()\n",
        "\n",
        "#PyTorch backward pass\n",
        "for p in parameters:\n",
        "    p.grad = None\n",
        "\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, norm_logits, logit_maxes, logits, h, h_pre_act, bnraw, bnvar_inv, bnvari, bndiff2, bndiff, h_pre_bn, bnmeani, emb_cat, emb]:\n",
        "    if t.requires_grad == False:\n",
        "        t.requires_grad = True\n",
        "    t.retain_grad()\n",
        "\n",
        "loss.backward()\n",
        "loss"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:29:04.044608Z",
          "iopub.execute_input": "2024-06-21T16:29:04.045122Z",
          "iopub.status.idle": "2024-06-21T16:29:04.070012Z",
          "shell.execute_reply.started": "2024-06-21T16:29:04.045067Z",
          "shell.execute_reply": "2024-06-21T16:29:04.06886Z"
        },
        "trusted": true,
        "id": "n96XK3QzutQf",
        "outputId": "59459c14-6ac5-4d8e-c1a3-e7a61cc8c0f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 338,
          "output_type": "execute_result",
          "data": {
            "text/plain": "tensor(3.3482, grad_fn=<NegBackward0>)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xb[0,2], C[4], emb[0,2]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:29:04.071702Z",
          "iopub.execute_input": "2024-06-21T16:29:04.072169Z",
          "iopub.status.idle": "2024-06-21T16:29:04.08412Z",
          "shell.execute_reply.started": "2024-06-21T16:29:04.072126Z",
          "shell.execute_reply": "2024-06-21T16:29:04.082544Z"
        },
        "trusted": true,
        "id": "z8a7jDahutQg",
        "outputId": "19976e71-ac59-4118-9091-7199c8de59aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 339,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(tensor(4),\n tensor([-0.9648, -0.2321, -0.3476,  0.3324, -1.3263,  1.1224,  0.5964,  0.4585,\n          0.0540, -1.7400], grad_fn=<SelectBackward0>),\n tensor([-0.9648, -0.2321, -0.3476,  0.3324, -1.3263,  1.1224,  0.5964,  0.4585,\n          0.0540, -1.7400], grad_fn=<SelectBackward0>))"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: backprop through the whole thing manually, backpropagating through exactly all of the variables as they are defined in the forward pass above, one by one"
      ],
      "metadata": {
        "id": "qnweVUzMutQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dlogprobs = torch.zeros_like(logprobs)\n",
        "dlogprobs[range(batch_size), Yb] = -1/batch_size\n",
        "\n",
        "dprobs = (1/probs) * dlogprobs\n",
        "\n",
        "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
        "dcounts = counts_sum_inv*dprobs\n",
        "\n",
        "dcounts_sum = (-1*counts_sum**-2) * dcounts_sum_inv\n",
        "dcounts +=  torch.ones_like(counts) * dcounts_sum\n",
        "\n",
        "dnorm_logits = counts * dcounts\n",
        "dlogit_maxes = (-1*dnorm_logits).sum(1, keepdim=True)\n",
        "dlogits = 1 * dnorm_logits + F.one_hot(logits.max(1).indices, num_classes=vocab_size) * dlogit_maxes\n",
        "\n",
        "dh = dlogits @ W2.T\n",
        "dW2 = h.T @ dlogits\n",
        "db2 = dlogits.sum(0)\n",
        "dh_pre_act = (1.0-h**2) * dh\n",
        "\n",
        "dbngain = (bnraw * dh_pre_act).sum(0, keepdim=True)\n",
        "dbnbias = dh_pre_act.sum(0, keepdim=True)\n",
        "\n",
        "dbnraw = bngain*dh_pre_act\n",
        "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "dbnvari = -0.5 * ((bnvari+1e-5)**(-1.5)) * dbnvar_inv\n",
        "\n",
        "dbndiff2 = 1/(batch_size-1) * torch.ones_like(bndiff2) * dbnvari\n",
        "dbndiff = bnvar_inv * dbnraw + 2*bndiff * dbndiff2\n",
        "\n",
        "dbnmeani = (-dbndiff).sum(0)\n",
        "dh_pre_bn = dbndiff + (1/batch_size)*torch.ones_like(h_pre_bn)*dbnmeani\n",
        "\n",
        "\n",
        "demb_cat = dh_pre_bn @ W1.T\n",
        "dW1 = emb_cat.T @ dh_pre_bn\n",
        "db1 = dh_pre_bn.sum(0)\n",
        "demb = demb_cat.view(emb.shape)\n",
        "\n",
        "dC = torch.zeros_like(C)\n",
        "for i in range(Xb.shape[0]):\n",
        "    for j in range(Xb.shape[1]):\n",
        "        ix = Xb[i,j]\n",
        "        dC[ix] += demb[i,j]\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:29:04.085915Z",
          "iopub.execute_input": "2024-06-21T16:29:04.086823Z",
          "iopub.status.idle": "2024-06-21T16:29:04.110393Z",
          "shell.execute_reply.started": "2024-06-21T16:29:04.086743Z",
          "shell.execute_reply": "2024-06-21T16:29:04.108995Z"
        },
        "trusted": true,
        "id": "9UAjepqPutQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPARISON\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "cmp('probs', dprobs, probs)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "cmp('counts', dcounts, counts)\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "cmp('logits', dlogits, logits)\n",
        "cmp('h', dh, h)\n",
        "cmp('W2', dW2, W2)\n",
        "cmp('b2', db2, b2)\n",
        "cmp('h_pre_act', dh_pre_act, h_pre_act)\n",
        "cmp('bngain', dbngain, bngain)\n",
        "cmp('bnbias', dbnbias, bnbias)\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "cmp('bnvari', dbnvari, bnvari)\n",
        "cmp('bndiff2', dbndiff2, bndiff2)\n",
        "cmp('bndiff', dbndiff, bndiff)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)\n",
        "cmp('h_pre_bn', dh_pre_bn, h_pre_bn)\n",
        "cmp('emb_cat', demb_cat, emb_cat)\n",
        "cmp('W1', dW1, W1)\n",
        "cmp('b1', db1, b1)\n",
        "cmp('emb', demb, emb)\n",
        "cmp('C', dC, C)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:29:04.112259Z",
          "iopub.execute_input": "2024-06-21T16:29:04.113083Z",
          "iopub.status.idle": "2024-06-21T16:29:04.131719Z",
          "shell.execute_reply.started": "2024-06-21T16:29:04.113043Z",
          "shell.execute_reply": "2024-06-21T16:29:04.130376Z"
        },
        "trusted": true,
        "id": "Amp_Wk45utQh",
        "outputId": "057c5c69-9e25-472c-cee5-e7bdc414193e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\nprobs           | exact: True  | approximate: True  | maxdiff: 0.0\ncounts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\ncounts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\ncounts          | exact: True  | approximate: True  | maxdiff: 0.0\nnorm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\nlogit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\nlogits          | exact: True  | approximate: True  | maxdiff: 0.0\nh               | exact: True  | approximate: True  | maxdiff: 0.0\nW2              | exact: True  | approximate: True  | maxdiff: 0.0\nb2              | exact: True  | approximate: True  | maxdiff: 0.0\nh_pre_act       | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\nbngain          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\nbnbias          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\nbnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\nbnvar_inv       | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\nbnvari          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\nbndiff2         | exact: False | approximate: True  | maxdiff: 1.4551915228366852e-11\nbndiff          | exact: False | approximate: True  | maxdiff: 6.984919309616089e-10\nbnmeani         | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\nh_pre_bn        | exact: False | approximate: True  | maxdiff: 6.984919309616089e-10\nemb_cat         | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\nW1              | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\nb1              | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\nemb             | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\nC               | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Backprop through cross_entropy in one expression"
      ],
      "metadata": {
        "id": "6t6K-dc9utQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass\n",
        "\n",
        "# before:\n",
        "\"\"\"\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes  #for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1\n",
        "probs = counts * counts_sum_inv\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(batch_size), Yb].mean()\n",
        "\"\"\"\n",
        "# now:\n",
        "loss_fast = F.cross_entropy(logits, Yb)\n",
        "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:29:04.133434Z",
          "iopub.execute_input": "2024-06-21T16:29:04.133922Z",
          "iopub.status.idle": "2024-06-21T16:29:04.142825Z",
          "shell.execute_reply.started": "2024-06-21T16:29:04.133879Z",
          "shell.execute_reply": "2024-06-21T16:29:04.141478Z"
        },
        "trusted": true,
        "id": "lFkFugm3utQh",
        "outputId": "2b49e2f4-d0c9-4643-c8eb-0481ac2ca8ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "3.348198175430298 diff: 0.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backward pass\n",
        "# -----------------\n",
        "# YOUR CODE HERE :)\n",
        "dlogits = F.softmax(logits, 1)\n",
        "dlogits[range(batch_size), Yb] -= 1\n",
        "dlogits /= batch_size\n",
        "# TODO. my solution is 3 lines\n",
        "# -----------------\n",
        "\n",
        "cmp('logits', dlogits, logits)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:29:04.147655Z",
          "iopub.execute_input": "2024-06-21T16:29:04.148585Z",
          "iopub.status.idle": "2024-06-21T16:29:04.156123Z",
          "shell.execute_reply.started": "2024-06-21T16:29:04.14855Z",
          "shell.execute_reply": "2024-06-21T16:29:04.154841Z"
        },
        "trusted": true,
        "id": "geHCBqFWutQi",
        "outputId": "b96eabef-3cf3-4559-a0c8-71834a514816"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "logits          | exact: False | approximate: True  | maxdiff: 8.614733815193176e-09\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: backprop through batchnorm but all in one go"
      ],
      "metadata": {
        "id": "0lruMkFFutQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass\n",
        "\n",
        "# before:\n",
        "\"\"\"\n",
        "bnmeani = (1/batch_size)*h_pre_bn.sum(0, keepdim=True)   # mean of the batch\n",
        "bndiff = h_pre_bn - bnmeani                              # difference between batch and batch mean\n",
        "bndiff2 = bndiff**2                                      # square of the difference between batch and mean for variance\n",
        "bnvari = 1/(batch_size-1) * bndiff2.sum(0, keepdim=True) # variance\n",
        "bnvar_inv = (bnvari+1e-5)**(-0.5)                        # epsilon = 1e-5\n",
        "bnraw = bndiff*bnvar_inv\n",
        "h_pre_act = bngain * bnraw + bnbias\n",
        "\"\"\"\n",
        "# now:\n",
        "h_pre_act_fast = bngain * (h_pre_bn-h_pre_bn.mean(0, keepdim=True))/(h_pre_bn.var(0, keepdim=True)+1e-5)**0.5 + bnbias\n",
        "print('max diff:', (h_pre_act_fast - h_pre_act).abs().max().item())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:29:04.157598Z",
          "iopub.execute_input": "2024-06-21T16:29:04.158068Z",
          "iopub.status.idle": "2024-06-21T16:29:04.167428Z",
          "shell.execute_reply.started": "2024-06-21T16:29:04.158023Z",
          "shell.execute_reply": "2024-06-21T16:29:04.166342Z"
        },
        "trusted": true,
        "id": "R2fvtNKyutQi",
        "outputId": "be4dc288-7987-4dd4-b114-c8eb61385a1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "max diff: 4.76837158203125e-07\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backward pass\n",
        "n = batch_size\n",
        "# before we had:\n",
        "\"\"\"\n",
        "dbngain = (bnraw * dh_pre_act).sum(0, keepdim=True)\n",
        "dbnbias = dh_pre_act.sum(0, keepdim=True)\n",
        "\n",
        "dbnraw = bngain*dh_pre_act\n",
        "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "dbnvari = -0.5 * ((bnvari+1e-5)**(-1.5)) * dbnvar_inv\n",
        "\n",
        "dbndiff2 = 1/(batch_size-1) * torch.ones_like(bndiff2) * dbnvari\n",
        "dbndiff = bnvar_inv * dbnraw + 2*bndiff * dbndiff2\n",
        "\n",
        "dbnmeani = (-dbndiff).sum(0)\n",
        "dh_pre_bn = dbndiff + (1/batch_size)*torch.ones_like(h_pre_bn)*dbnmeani\n",
        "\"\"\"\n",
        "\n",
        "# -----------------\n",
        "# YOUR CODE HERE :)\n",
        "dh_pre_bn = bngain*bnvar_inv/n * (n*dh_pre_act - dh_pre_act.sum(0) - n/(n-1)*bnraw*(dh_pre_act*bnraw).sum(0))\n",
        "# -----------------\n",
        "\n",
        "cmp('hprebn', dh_pre_bn, h_pre_bn) # I can only get approximate to be true, my maxdiff is 9e-10"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:29:04.16904Z",
          "iopub.execute_input": "2024-06-21T16:29:04.169412Z",
          "iopub.status.idle": "2024-06-21T16:29:04.184806Z",
          "shell.execute_reply.started": "2024-06-21T16:29:04.169381Z",
          "shell.execute_reply": "2024-06-21T16:29:04.183335Z"
        },
        "trusted": true,
        "id": "G8fsPz_uutQj",
        "outputId": "da3198b6-dedc-4214-bf7d-561db711b1e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4: Putting it all together"
      ],
      "metadata": {
        "id": "RlH7P2C6utQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training a MLP with our backward pass\n",
        "\n",
        "n_embed = 10\n",
        "n_hidden = 200\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) #for deterministic and reproducable results\n",
        "C = torch.randn((vocab_size, n_embed),           generator=g)\n",
        "#Layer 1\n",
        "W1 = torch.randn((n_embed*block_size, n_hidden), generator=g) * (5/3)/((n_embed*block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden, generator=g) * 0.1 #useless\n",
        "#Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),         generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                     generator=g) * 0.1\n",
        "#BatchNorm parameters\n",
        "bngain = torch.ones((1, n_hidden))*0.1+1.0\n",
        "bnbias = torch.zeros((1, n_hidden))*0.1\n",
        "#Mean and std buffers, (not trainable)\n",
        "bnmean_running = torch.zeros((1, n_hidden)) * 0.1 + 1\n",
        "bnstd_running = torch.ones((1, n_hidden)) * 0.1\n",
        "\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters))\n",
        "for p in parameters:\n",
        "    p.requires_grad = True\n",
        "\n",
        "\n",
        "#optimization\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "n = batch_size # convenience\n",
        "lossi = []\n",
        "\n",
        "# use this context manager for efficiency once your backward pass is written (TODO)\n",
        "with torch.no_grad():\n",
        "\n",
        "    for i in range(max_steps):\n",
        "\n",
        "    #     minibatch construct\n",
        "        ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "        Xb, Yb = Xtr[ix], Ytr[ix]\n",
        "\n",
        "    #     embedding\n",
        "        emb = C[Xb]\n",
        "        embcat = emb.view(emb.shape[0], -1)\n",
        "\n",
        "        # ----- Linear Layer 1 -----\n",
        "        # hidden layer pre-activation\n",
        "        hprebn = embcat @ W1 + b1\n",
        "        # ---- BatchNorm layer ----\n",
        "        # mean of hidden layer pre-activation\n",
        "        bnmeani = 1 / n * hprebn.sum(0, keepdim = True)\n",
        "        # difference between pre-activation and mean\n",
        "        bndiff = hprebn - bnmeani\n",
        "        # square of difference\n",
        "        bndiff2 = bndiff ** 2\n",
        "        # variance of hidden layer pre-activation (Bessel's correction (n-1))\n",
        "        bnvar = 1 / (n - 1) * (bndiff2).sum(0, keepdim = True)\n",
        "        # get the std of hidden layer pre-activation, and invert it (remember: we need to divide by it)\n",
        "        bnvar_inv = 1 / (bnvar + 1e-5) ** 0.5\n",
        "        # batch norm layer output\n",
        "        bnraw = bndiff * bnvar_inv\n",
        "        # batch norm layer output with gain and bias (scale and shift)\n",
        "        hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "        with torch.no_grad():\n",
        "            bnmean_running = 0.99*bnmean_running * 0.01*bnmeani\n",
        "            bnstd_running = 0.99*bnstd_running * 0.01*bnvar**0.5\n",
        "        # activation function\n",
        "        h = torch.tanh(hpreact)\n",
        "\n",
        "\n",
        "        # ----- Linear Layer 2 -----\n",
        "        # logits\n",
        "        logits = h @ W2 + b2\n",
        "\n",
        "        # ----- Cross Entropy Loss -----\n",
        "        loss = F.cross_entropy(logits, Yb)\n",
        "\n",
        "        # ----- Backpropagation and Zero_grad-----\n",
        "        for p in parameters:\n",
        "            p.grad = None\n",
        "#         loss.backward()\n",
        "\n",
        "        dlogits = F.softmax(logits, 1)\n",
        "        dlogits[range(n), Yb] -= 1\n",
        "        dlogits /= n\n",
        "        # 2nd layer backprop\n",
        "        dh = dlogits @ W2.T\n",
        "        dW2 = h.T @ dlogits\n",
        "        db2 = dlogits.sum(0)\n",
        "        # tanh\n",
        "        dhpreact = (1.0 - h**2) * dh\n",
        "        # batchnorm backprop\n",
        "        dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "        dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "        dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "        # 1st layer\n",
        "        dembcat = dhprebn @ W1.T\n",
        "        dW1 = embcat.T @ dhprebn\n",
        "        db1 = dhprebn.sum(0)\n",
        "        # embedding\n",
        "        demb = dembcat.view(emb.shape)\n",
        "        dC = torch.zeros_like(C)\n",
        "        for k in range(Xb.shape[0]):\n",
        "            for j in range(Xb.shape[1]):\n",
        "                ix = Xb[k,j]\n",
        "                dC[ix] += demb[k,j]\n",
        "        grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
        "\n",
        "        # update\n",
        "        lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "        for p, grad in zip(parameters, grads):\n",
        "          #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
        "          p.data += -lr * grad # new way\n",
        "\n",
        "        # track stats\n",
        "        if i % 10000 == 0: # print every once in a while\n",
        "            print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "        lossi.append(loss.log10().item())\n",
        "\n",
        "#     if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
        "#         break"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:29:04.18682Z",
          "iopub.execute_input": "2024-06-21T16:29:04.187283Z",
          "iopub.status.idle": "2024-06-21T16:39:14.697717Z",
          "shell.execute_reply.started": "2024-06-21T16:29:04.18725Z",
          "shell.execute_reply": "2024-06-21T16:39:14.695905Z"
        },
        "trusted": true,
        "id": "X2aR2eUUutQj",
        "outputId": "d48958c6-15cb-4d63-c1a0-102555c9ebf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "12297\n      0/ 200000: 3.8279\n  10000/ 200000: 2.1609\n  20000/ 200000: 2.4227\n  30000/ 200000: 2.4362\n  40000/ 200000: 2.0088\n  50000/ 200000: 2.4084\n  60000/ 200000: 2.4508\n  70000/ 200000: 2.1090\n  80000/ 200000: 2.3592\n  90000/ 200000: 2.2353\n 100000/ 200000: 1.9750\n 110000/ 200000: 2.3438\n 120000/ 200000: 2.0156\n 130000/ 200000: 2.4772\n 140000/ 200000: 2.3107\n 150000/ 200000: 2.1108\n 160000/ 200000: 1.9497\n 170000/ 200000: 1.8004\n 180000/ 200000: 2.0284\n 190000/ 200000: 1.8848\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # useful for checking your gradients\n",
        "# for p,g in zip(parameters, grads):\n",
        "#     cmp(str(tuple(p.shape)), g, p)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:39:14.702322Z",
          "iopub.execute_input": "2024-06-21T16:39:14.702747Z",
          "iopub.status.idle": "2024-06-21T16:39:14.708875Z",
          "shell.execute_reply.started": "2024-06-21T16:39:14.702712Z",
          "shell.execute_reply": "2024-06-21T16:39:14.70742Z"
        },
        "trusted": true,
        "id": "qZ1_CG5putQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    emb = C[Xtr]\n",
        "    embcat = emb.view(emb.shape[0],-1)\n",
        "    hpreact = embcat @ W1 + b1\n",
        "\n",
        "    bnmean = hpreact.mean(0, keepdim=True)\n",
        "    bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:39:14.710735Z",
          "iopub.execute_input": "2024-06-21T16:39:14.711248Z",
          "iopub.status.idle": "2024-06-21T16:39:15.227708Z",
          "shell.execute_reply.started": "2024-06-21T16:39:14.711194Z",
          "shell.execute_reply": "2024-06-21T16:39:15.226247Z"
        },
        "trusted": true,
        "id": "n7s7yTUwutQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def split_loss(split):\n",
        "    x, y = {\"train\" : (Xtr, Ytr), \"dev\" : (Xval, Yval),\"test\": (Xtest, Ytest)}[split]\n",
        "    emb = C[x] #shape: (No of examples/batch_size, block_size, embeddings)\n",
        "    embcat = emb.view(emb.shape[0], -1)\n",
        "    hpreact = embcat @ W1 + b1\n",
        "    hpreact = bnbias * (hpreact-bnmean) * (bnvar+1e-5)**-0.5 + bnbias\n",
        "    h = torch.tanh(hpreact) #shape: (batch_size, no_hidden)\n",
        "    logits = h @ W2 + b2 #shape: (batch_size, vocab_size)\n",
        "\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "    print(f\"{split} : {loss.item()}\")\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('dev')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:53:29.94242Z",
          "iopub.execute_input": "2024-06-21T16:53:29.942946Z",
          "iopub.status.idle": "2024-06-21T16:53:30.64125Z",
          "shell.execute_reply.started": "2024-06-21T16:53:29.942907Z",
          "shell.execute_reply": "2024-06-21T16:53:30.640063Z"
        },
        "trusted": true,
        "id": "ZCnaSGekutQk",
        "outputId": "474d79a4-2303-48dc-fdec-e5d30848dbcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "train : 3.4889321327209473\ndev : 3.4822936058044434\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate train and val loss\n",
        "\n",
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "    x,y = {'train': (Xtr, Ytr), 'val': (Xval, Yval),'test': (Xtest, Ytest),}[split]\n",
        "    emb = C[x] # (N, block_size, n_embd)\n",
        "    embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "    hpreact = embcat @ W1 + b1\n",
        "    hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "    h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "    logits = h @ W2 + b2 # (N, vocab_size)\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "    print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:47:34.448263Z",
          "iopub.execute_input": "2024-06-21T16:47:34.448698Z",
          "iopub.status.idle": "2024-06-21T16:47:35.136548Z",
          "shell.execute_reply.started": "2024-06-21T16:47:34.448663Z",
          "shell.execute_reply": "2024-06-21T16:47:35.13539Z"
        },
        "trusted": true,
        "id": "O2XY5v3iutQk",
        "outputId": "b173cf8d-866e-4a22-b4c8-21d98628db04"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "train 2.0673816204071045\nval 2.1062190532684326\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train 2.0673816204071045\n",
        "### val 2.1062190532684326"
      ],
      "metadata": {
        "id": "8WppUzaDutQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for i in range(20):\n",
        "    out = []\n",
        "    context =[0]*block_size\n",
        "\n",
        "    while True:\n",
        "        emb = C[torch.tensor([context])] # (1,block_size,d)\n",
        "        embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "        hpreact = embcat @ W1 + b1\n",
        "        hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "        h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "        logits = h @ W2 + b2 # (N, vocab_size)\n",
        "\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "        context = context[1:]+[ix]\n",
        "        out.append(ix)\n",
        "        if ix==0:\n",
        "            break\n",
        "\n",
        "    print(''.join(itos[i] for i in out))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:39:55.406439Z",
          "iopub.execute_input": "2024-06-21T16:39:55.406935Z",
          "iopub.status.idle": "2024-06-21T16:39:55.473167Z",
          "shell.execute_reply.started": "2024-06-21T16:39:55.406889Z",
          "shell.execute_reply": "2024-06-21T16:39:55.472018Z"
        },
        "trusted": true,
        "id": "soFyw8i0utQl",
        "outputId": "ad66f0cf-4e26-4c8e-adc8-73e6c98b8325"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "mora.\nmayah.\nsee.\nmad.\nryla.\nren.\nruthadraegen.\nchedielin.\nshi.\njenleigh.\nsananaraelyn.\nmalaia.\nnoshubergihirael.\nkindreth.\nkonnie.\ncayu.\nzayven.\njamyleyeh.\nyuma.\nmyston.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zjahPFGuutQn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}